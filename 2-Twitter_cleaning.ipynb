{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "banks\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import feather\n",
    "import string\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "dataset = input()\n",
    "path = os.getcwd() + '/Datasets/'+dataset+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1537286, 15)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_feather('Datasets/#fintech.csv')\n",
    "df = pd.read_feather(path+dataset+'18_21.ftr')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4502569, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Append 15-18 tweets\n",
    "df_old = pd.read_feather(path+dataset+'15_18.ftr')\n",
    "df = df.append(df_old, ignore_index = True)\n",
    "# Append 09-15 tweets\n",
    "df_old = pd.read_feather(path+dataset+'09_15.ftr')\n",
    "df = df.append(df_old, ignore_index = True)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Filter and Clean the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4018801, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Next, mostly to get rid of bots and cryptocurrency warriors, \n",
    "#we are going to remove users that have less than 100 followers:\n",
    "df['sum_pubmetrics'] = df['public_metrics.retweet_count'] + df['public_metrics.like_count']\n",
    "df = df[df['public_metrics.followers_count']>=100]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset the index to start from 0 and drop the previous index columns\n",
    "df = df.reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For banking create a dummy variable that shows whether the tweet contains the word/hashtag fintech\n",
    "df['fintech'] = df.text.str.contains('fintech', case = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_feather(path+dataset+'_full.ftr')\n",
    "df.to_csv(path+dataset+'_full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Clean the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_round1(text):\n",
    "    \"\"\"Initial cleaning I make to the dataset\"\"\"\n",
    "    text = re.sub(r'@[A-Za-z0-9]+','',text) #Removes @mentions\n",
    "    text = re.sub(r'#','',text) #Removes the '#' symbol\n",
    "    text = re.sub(r'&amp;','',text) #Removes &amp\n",
    "    text = re.sub(r'https?:\\/\\/\\S+','',text) #Remove the hyper link\n",
    "    text = re.sub(r'\\n','',text) #remove the new line symbol\n",
    "    text = re.sub(r'\\t','',text) #remove the tab symbol\n",
    "    text = re.sub(r'-',' ',text) #removes the dash and replaces it with space\n",
    "    #Removes the punctuation:\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text) #remove words containing numbers\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]+\", \" \", text) #Removes special characters except space\n",
    "    text = re.sub(' +', ' ', text).strip() #Removes trailing white spaces\n",
    "    text = text.lower() # lowers the text\n",
    "    text = re.sub(r\"(.)\\1{2,}\",r\"\\1\", text) # Replace triple+ characters with single char\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clean_text_round1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Organize the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For corpus I will group by years and make separate corpuses/columns for different years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_feather(path+dataset+'_clean.ftr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index to be the date the tweet was created\n",
    "df = df.set_index('tweet_created_at')\n",
    "# Group the tweets by year and merge the tweets by separating them by space:\n",
    "df = df.text.groupby(pd.Grouper(freq='Y')).apply(' '.join)\n",
    "# Rename the index to year ending in:\n",
    "df = df.rename_axis('year_ending_in')\n",
    "# Save it as a corpus:\n",
    "#df.to_frame().reset_index().to_feather('Datasets/#'+dataset+'_corpus.ftr')\n",
    "df.to_pickle(path+dataset+'_corpus.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the document-term matrix is:\n",
      "(13, 789413)\n"
     ]
    }
   ],
   "source": [
    "# Create a pandas dataframe: columns are the words appearing in the tweets,\n",
    "# rows are showing the frequency of these words\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "# Set the index to be dates:\n",
    "data_dtm.index = df.index\n",
    "\n",
    "print('The shape of the document-term matrix is:')\n",
    "print(data_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dtm:\n",
    "data_dtm.transpose().to_pickle(path+dataset+'_dtm.pkl')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
