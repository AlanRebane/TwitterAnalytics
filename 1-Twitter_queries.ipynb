{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import urllib.parse\n",
    "import config\n",
    "import pandas as pd\n",
    "import feather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Creating the API connection and defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load the bearer token from the config.py file\n",
    "bearer_token = config.bearer_token\n",
    "#Create the headers for authorization using the bearer token.\n",
    "headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "\n",
    "def main(query, fields, headers, print_yes=True):\n",
    "    \"\"\"\n",
    "    This function loads up the bearer token, defines the query url\n",
    "    and headers and returns the API response in JSON.\n",
    "    \n",
    "    Inputs:\n",
    "    query = String variable. This is the main query specifying \n",
    "            the Query Parameters. For example: from:elonmusk, lang:en\n",
    "            -is:retweet, max_results=100. Space separated list.\n",
    "    tweet_fields = String variable. This defines what Response\n",
    "            Fields we want for the tweets. For example: author_id, \n",
    "            public_metrics, created_at, geo, id. Comma separated list\n",
    "            without spaces.\n",
    "    headers = Authorization containing the bearer token. Dictionary.\n",
    "    search = Specifies whether you want to query for 'users' or 'tweets'.\n",
    "            String variable.\n",
    "    print_yes = Boolean whether you want to print the output.\n",
    "    \n",
    "    Returns:\n",
    "    json response object containing the API response\n",
    "    \"\"\"\n",
    "    #Create the url for the API request.\n",
    "    query = urllib.parse.quote(query)\n",
    "    #Create a query and fields URL that will be fed into the API request.\n",
    "    url = \"https://api.twitter.com/2/tweets/search/all?query={}&{}\".format(query, fields)\n",
    "    \n",
    "    if print_yes == True:\n",
    "        print(url)\n",
    "    \n",
    "    #Connect to the Twitter API endpoint using the query url and headers\n",
    "    response = requests.request(\"GET\", url, headers=headers)\n",
    "    if print_yes == True:\n",
    "        print(response.status_code)\n",
    "    \n",
    "    #If Twitter's servers are overloaded, you may need to wait a before you make a request\n",
    "    if response.status_code == 503:\n",
    "        print(\"Twitter's service is currently unavailable, I will try again in five minutes.\")\n",
    "        time.sleep(300)\n",
    "        response = requests.request(\"GET\", url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    \n",
    "    if print_yes == True:\n",
    "        print(json.dumps(response.json(), indent=4, sort_keys=True))\n",
    "    \n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getRateLimit(headers):\n",
    "    \"\"\"\n",
    "    This functions inputs the headers (the bearer token)\n",
    "    and outputs how many tweets I have in the 15 minute window\n",
    "    before the rate limit is hit and the timestamp when the rate is reset\n",
    "    \"\"\"\n",
    "    url='https://api.twitter.com/1.1/application/rate_limit_status.json?'\n",
    "    response = requests.request(\"GET\",url,headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    \n",
    "    return response.json()['resources']['tweets']['/tweets/search/all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_clean(df, df_users):\n",
    "    #Merge the two dataframes together:\n",
    "    df_cleaned = df.merge(df_users, left_on='author_id', right_on='id')\n",
    "    #Drop unneccessary columns:\n",
    "    df_cleaned = df_cleaned.drop(['id_x', 'author_id', 'id_y'], axis=1)\n",
    "    #Rename some columns:\n",
    "    df_cleaned = df_cleaned.rename(columns = {'created_at_x': 'tweet_created_at',\n",
    "                                              'created_at_y': 'user_created_at'})\n",
    "    #Convert the dates to YYYY-MM-DD format so that they are easier to handle later on:\n",
    "    df_cleaned['tweet_created_at'] = df_cleaned['tweet_created_at'].astype('datetime64[ns]')\n",
    "    df_cleaned['user_created_at'] = df_cleaned['user_created_at'].astype('datetime64[ns]')\n",
    "    df_cleaned['tweet_created_at'] = [time.date() for time in df_cleaned['tweet_created_at']]\n",
    "    df_cleaned['user_created_at'] = [time.date() for time in df_cleaned['user_created_at']]\n",
    "    df_cleaned['tweet_created_at'] = df_cleaned['tweet_created_at'].astype('datetime64[ns]')\n",
    "    df_cleaned['user_created_at'] = df_cleaned['user_created_at'].astype('datetime64[ns]')\n",
    "    \n",
    "    #Convert int64 to in32 to save space:\n",
    "    df_cleaned['public_metrics.retweet_count'] = df_cleaned['public_metrics.retweet_count'].astype('int32')\n",
    "    df_cleaned['public_metrics.reply_count'] = df_cleaned['public_metrics.reply_count'].astype('int32')\n",
    "    df_cleaned['public_metrics.like_count'] = df_cleaned['public_metrics.like_count'].astype('int32')\n",
    "    df_cleaned['public_metrics.quote_count'] = df_cleaned['public_metrics.quote_count'].astype('int32')\n",
    "    df_cleaned['public_metrics.followers_count'] = df_cleaned['public_metrics.followers_count'].astype('int32')\n",
    "    df_cleaned['public_metrics.following_count'] = df_cleaned['public_metrics.following_count'].astype('int32')\n",
    "    df_cleaned['public_metrics.tweet_count'] = df_cleaned['public_metrics.tweet_count'].astype('int32')\n",
    "    df_cleaned['public_metrics.listed_count'] = df_cleaned['public_metrics.listed_count'].astype('int32')\n",
    "    \n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Download the whole dataset to the local computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getRateLimit(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the query and the fields\n",
    "query = '#banking lang:en -is:retweet'\n",
    "fields_orig = \"tweet.fields=author_id,created_at,public_metrics\"\\\n",
    "              \"&expansions=author_id\"\\\n",
    "              \"&user.fields=username,verified,created_at,public_metrics,description\"\\\n",
    "              \"&max_results=500\"\\\n",
    "              \"&start_time=2009-01-01T00%3A00%3A00Z\"\\\n",
    "              \"&end_time=2013-01-01T00%3A00%3A00Z\"\n",
    "\n",
    "#Call the API and define the dataframe for the tweets and the users:\n",
    "result = main(query, fields_orig, headers, print_yes = False)\n",
    "#Create dataframes from the result output, df corresponds to tweets and df_users to users\n",
    "df = pd.json_normalize(result['data'])\n",
    "df_users = pd.json_normalize(result['includes']['users'])\n",
    "df_merged = merge_and_clean(df, df_users)\n",
    "\n",
    "#Get the next_token from the API response\n",
    "next_token = result['meta']['next_token']\n",
    "\n",
    "#To keep track of how many requests I have made\n",
    "index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cycle = True\n",
    "print(f'You currently have {df_merged.shape[0]} tweets in your dataframe \\n')\n",
    "\n",
    "try:\n",
    "    while cycle is True:\n",
    "        #Update the fields with the new next_token:\n",
    "        fields = fields_orig + '&next_token=' + next_token\n",
    "        \n",
    "        result = main(query, fields, headers, print_yes = False)\n",
    "        \n",
    "        #Define temporary dataframes that will be appended to the main dataframes:\n",
    "        df_temp = pd.json_normalize(result['data'])\n",
    "        df_user_temp = pd.json_normalize(result['includes']['users'])\n",
    "        df_merged_temp = merge_and_clean(df_temp, df_user_temp)\n",
    "        #Append the merged dataframe (tweets&users) to the main merged dataframe:\n",
    "        df_merged = df_merged.append(df_merged_temp, ignore_index = True)\n",
    "        \n",
    "        #Requests are limited to 1 request per 1 second:\n",
    "        time.sleep(1)\n",
    "        \n",
    "        #Update the index after making another request:\n",
    "        index += 1\n",
    "        if index%100==0:\n",
    "            print(f'I have made {index} requests and downloaded {df_merged.shape[0]} tweets')\n",
    "            print('Dataframe length:', df_merged.shape[0])\n",
    "            #Provide the last date in the dataframe to know where I'm at\n",
    "            last_date = str(df_merged.iloc[-1, df_merged.columns.get_loc('tweet_created_at')])[:10]\n",
    "            print('Dataframe last date:', last_date)\n",
    "            #Calculate the total size of the dataframe:\n",
    "            size = df_merged.memory_usage(deep=True).sum()/1000\n",
    "            print('Dataframe size (kb):', size, '\\n')\n",
    "            \n",
    "        #Check how far you are from the rate limit:    \n",
    "        if index%10==0:\n",
    "            rate_limit = getRateLimit(headers)\n",
    "            #If there are less than 21 requests to the limit, sleep a bit:\n",
    "            if rate_limit['remaining']<11:\n",
    "                #Calculate the sleep time:\n",
    "                sleep_time = int((rate_limit['reset']-time.time())+5)\n",
    "                print(f'I have made {index} requests and will now sleep for {int(sleep_time)} seconds \\n')\n",
    "                time.sleep(sleep_time)\n",
    "                \n",
    "        #Get the next token:\n",
    "        next_token = result['meta']['next_token']\n",
    "\n",
    "\n",
    "except KeyError as e:\n",
    "    print('There is no', str(e))\n",
    "\n",
    "finally:\n",
    "    print(f'You have downloaded {df_merged.shape[0]} tweets and the loop is now finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged.drop(['withheld.copyright', 'withheld.country_codes_x', \n",
    "                            'withheld.country_codes_y',], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv('Datasets/#banking09-12.csv')\n",
    "df_merged.to_pickle('Datasets/#banking09-12.pickle')\n",
    "df_merged.to_feather('Datasets/#banking09-12.ftr')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
