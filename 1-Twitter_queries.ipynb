{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import urllib.parse\n",
    "import config\n",
    "import pandas as pd\n",
    "import feather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Creating the API connection and defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load the bearer token from the config.py file\n",
    "bearer_token = config.bearer_token\n",
    "#Create the headers for authorization using the bearer token.\n",
    "headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "\n",
    "def main(query, fields, headers, print_yes=True):\n",
    "    \"\"\"\n",
    "    This function loads up the bearer token, defines the query url\n",
    "    and headers and returns the API response in JSON.\n",
    "    \n",
    "    Inputs:\n",
    "    query = String variable. This is the main query specifying \n",
    "            the Query Parameters. For example: from:elonmusk, lang:en\n",
    "            -is:retweet, max_results=100. Space separated list.\n",
    "    tweet_fields = String variable. This defines what Response\n",
    "            Fields we want for the tweets. For example: author_id, \n",
    "            public_metrics, created_at, geo, id. Comma separated list\n",
    "            without spaces.\n",
    "    headers = Authorization containing the bearer token. Dictionary.\n",
    "    search = Specifies whether you want to query for 'users' or 'tweets'.\n",
    "            String variable.\n",
    "    print_yes = Boolean whether you want to print the output.\n",
    "    \n",
    "    Returns:\n",
    "    json response object containing the API response\n",
    "    \"\"\"\n",
    "    #Create the url for the API request.\n",
    "    query = urllib.parse.quote(query)\n",
    "    #Create a query and fields URL that will be fed into the API request.\n",
    "    url = \"https://api.twitter.com/2/tweets/search/all?query={}&{}\".format(query, fields)\n",
    "    \n",
    "    if print_yes == True:\n",
    "        print(url)\n",
    "    \n",
    "    #Connect to the Twitter API endpoint using the query url and headers\n",
    "    response = requests.request(\"GET\", url, headers=headers)\n",
    "    if print_yes == True:\n",
    "        print(response.status_code)\n",
    "    \n",
    "    #If Twitter's servers are overloaded, you may need to wait a before you make a request\n",
    "    if response.status_code == 503:\n",
    "        print(\"Twitter's service is currently unavailable, I will try again in five minutes.\")\n",
    "        time.sleep(300)\n",
    "        response = requests.request(\"GET\", url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    \n",
    "    if print_yes == True:\n",
    "        print(json.dumps(response.json(), indent=4, sort_keys=True))\n",
    "    \n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getRateLimit(headers):\n",
    "    \"\"\"\n",
    "    This functions inputs the headers (the bearer token)\n",
    "    and outputs how many tweets I have in the 15 minute window\n",
    "    before the rate limit is hit and the timestamp when the rate is reset\n",
    "    \"\"\"\n",
    "    url='https://api.twitter.com/1.1/application/rate_limit_status.json?'\n",
    "    response = requests.request(\"GET\",url,headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    \n",
    "    return response.json()['resources']['tweets']['/tweets/search/all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_clean(df, df_users):\n",
    "    #Merge the two dataframes together:\n",
    "    df_cleaned = df.merge(df_users, left_on='author_id', right_on='id')\n",
    "    #Drop unneccessary columns:\n",
    "    df_cleaned = df_cleaned.drop(['id_x', 'author_id', 'id_y'], axis=1)\n",
    "    #Rename some columns:\n",
    "    df_cleaned = df_cleaned.rename(columns = {'created_at_x': 'tweet_created_at',\n",
    "                                              'created_at_y': 'user_created_at'})\n",
    "    #Convert the dates to YYYY-MM-DD format so that they are easier to handle later on:\n",
    "    df_cleaned['tweet_created_at'] = df_cleaned['tweet_created_at'].astype('datetime64[ns]')\n",
    "    df_cleaned['user_created_at'] = df_cleaned['user_created_at'].astype('datetime64[ns]')\n",
    "    df_cleaned['tweet_created_at'] = [time.date() for time in df_cleaned['tweet_created_at']]\n",
    "    df_cleaned['user_created_at'] = [time.date() for time in df_cleaned['user_created_at']]\n",
    "    df_cleaned['tweet_created_at'] = df_cleaned['tweet_created_at'].astype('datetime64[ns]')\n",
    "    df_cleaned['user_created_at'] = df_cleaned['user_created_at'].astype('datetime64[ns]')\n",
    "    \n",
    "    #Convert int64 to in32 to save space:\n",
    "    df_cleaned['public_metrics.retweet_count'] = df_cleaned['public_metrics.retweet_count'].astype('int32')\n",
    "    df_cleaned['public_metrics.reply_count'] = df_cleaned['public_metrics.reply_count'].astype('int32')\n",
    "    df_cleaned['public_metrics.like_count'] = df_cleaned['public_metrics.like_count'].astype('int32')\n",
    "    df_cleaned['public_metrics.quote_count'] = df_cleaned['public_metrics.quote_count'].astype('int32')\n",
    "    df_cleaned['public_metrics.followers_count'] = df_cleaned['public_metrics.followers_count'].astype('int32')\n",
    "    df_cleaned['public_metrics.following_count'] = df_cleaned['public_metrics.following_count'].astype('int32')\n",
    "    df_cleaned['public_metrics.tweet_count'] = df_cleaned['public_metrics.tweet_count'].astype('int32')\n",
    "    df_cleaned['public_metrics.listed_count'] = df_cleaned['public_metrics.listed_count'].astype('int32')\n",
    "    \n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Download the whole dataset to the local computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'limit': 300, 'remaining': 211, 'reset': 1619181917}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getRateLimit(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the query and the fields\n",
    "query = '(#banking OR #banks OR #bank) -\"RT\" lang:en -is:retweet -has:cashtags'\n",
    "fields_orig = \"tweet.fields=author_id,created_at,public_metrics\"\\\n",
    "              \"&expansions=author_id\"\\\n",
    "              \"&user.fields=username,verified,created_at,public_metrics,description\"\\\n",
    "              \"&max_results=500\"\\\n",
    "              \"&start_time=2009-01-01T00%3A00%3A00Z\"\\\n",
    "              \"&end_time=2015-01-01T00%3A00%3A00Z\"\n",
    "\n",
    "#Call the API and define the dataframe for the tweets and the users:\n",
    "result = main(query, fields_orig, headers, print_yes = False)\n",
    "#Create dataframes from the result output, df corresponds to tweets and df_users to users\n",
    "df = pd.json_normalize(result['data'])\n",
    "df_users = pd.json_normalize(result['includes']['users'])\n",
    "df_merged = merge_and_clean(df, df_users)\n",
    "\n",
    "#Get the next_token from the API response\n",
    "next_token = result['meta']['next_token']\n",
    "\n",
    "#To keep track of how many requests I have made\n",
    "index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You currently have 457 tweets in your dataframe \n",
      "\n",
      "I have made 100 requests and downloaded 47010 tweets\n",
      "Dataframe length: 47010\n",
      "Dataframe last date: 2014-11-12\n",
      "Dataframe size (kb): 26123.667 \n",
      "\n",
      "I have made 200 requests and downloaded 94006 tweets\n",
      "Dataframe length: 94006\n",
      "Dataframe last date: 2014-09-26\n",
      "Dataframe size (kb): 52410.07 \n",
      "\n",
      "I have made 300 requests and downloaded 140870 tweets\n",
      "Dataframe length: 140870\n",
      "Dataframe last date: 2014-08-03\n",
      "Dataframe size (kb): 78697.267 \n",
      "\n",
      "I have made 370 requests and will now sleep for 245 seconds \n",
      "\n",
      "I have made 400 requests and downloaded 187629 tweets\n",
      "Dataframe length: 187629\n",
      "Dataframe last date: 2014-06-09\n",
      "Dataframe size (kb): 104709.865 \n",
      "\n",
      "I have made 500 requests and downloaded 233998 tweets\n",
      "Dataframe length: 233998\n",
      "Dataframe last date: 2014-04-08\n",
      "Dataframe size (kb): 153220.007 \n",
      "\n",
      "I have made 600 requests and downloaded 281541 tweets\n",
      "Dataframe length: 281541\n",
      "Dataframe last date: 2014-02-03\n",
      "Dataframe size (kb): 184367.091 \n",
      "\n",
      "I have made 660 requests and will now sleep for 208 seconds \n",
      "\n",
      "I have made 700 requests and downloaded 328350 tweets\n",
      "Dataframe length: 328350\n",
      "Dataframe last date: 2013-11-25\n",
      "Dataframe size (kb): 214986.66 \n",
      "\n",
      "I have made 800 requests and downloaded 375224 tweets\n",
      "Dataframe length: 375224\n",
      "Dataframe last date: 2013-09-25\n",
      "Dataframe size (kb): 245552.663 \n",
      "\n",
      "I have made 900 requests and downloaded 423104 tweets\n",
      "Dataframe length: 423104\n",
      "Dataframe last date: 2013-07-24\n",
      "Dataframe size (kb): 276592.833 \n",
      "\n",
      "I have made 950 requests and will now sleep for 197 seconds \n",
      "\n",
      "I have made 1000 requests and downloaded 470908 tweets\n",
      "Dataframe length: 470908\n",
      "Dataframe last date: 2013-05-28\n",
      "Dataframe size (kb): 307632.651 \n",
      "\n",
      "I have made 1100 requests and downloaded 518801 tweets\n",
      "Dataframe length: 518801\n",
      "Dataframe last date: 2013-04-11\n",
      "Dataframe size (kb): 337720.905 \n",
      "\n",
      "I have made 1200 requests and downloaded 566827 tweets\n",
      "Dataframe length: 566827\n",
      "Dataframe last date: 2013-03-04\n",
      "Dataframe size (kb): 367700.754 \n",
      "\n",
      "I have made 1240 requests and will now sleep for 184 seconds \n",
      "\n",
      "I have made 1300 requests and downloaded 615497 tweets\n",
      "Dataframe length: 615497\n",
      "Dataframe last date: 2013-01-21\n",
      "Dataframe size (kb): 398086.23 \n",
      "\n",
      "I have made 1400 requests and downloaded 663932 tweets\n",
      "Dataframe length: 663932\n",
      "Dataframe last date: 2012-11-30\n",
      "Dataframe size (kb): 427841.807 \n",
      "\n",
      "I have made 1500 requests and downloaded 712334 tweets\n",
      "Dataframe length: 712334\n",
      "Dataframe last date: 2012-10-11\n",
      "Dataframe size (kb): 457624.666 \n",
      "\n",
      "I have made 1530 requests and will now sleep for 166 seconds \n",
      "\n",
      "I have made 1600 requests and downloaded 761313 tweets\n",
      "Dataframe length: 761313\n",
      "Dataframe last date: 2012-08-22\n",
      "Dataframe size (kb): 487572.065 \n",
      "\n",
      "I have made 1700 requests and downloaded 809961 tweets\n",
      "Dataframe length: 809961\n",
      "Dataframe last date: 2012-07-06\n",
      "Dataframe size (kb): 517424.101 \n",
      "\n",
      "I have made 1800 requests and downloaded 858491 tweets\n",
      "Dataframe length: 858491\n",
      "Dataframe last date: 2012-05-23\n",
      "Dataframe size (kb): 547273.33 \n",
      "\n",
      "I have made 1820 requests and will now sleep for 151 seconds \n",
      "\n",
      "I have made 1900 requests and downloaded 907204 tweets\n",
      "Dataframe length: 907204\n",
      "Dataframe last date: 2012-03-30\n",
      "Dataframe size (kb): 576972.663 \n",
      "\n",
      "I have made 2000 requests and downloaded 955821 tweets\n",
      "Dataframe length: 955821\n",
      "Dataframe last date: 2012-01-23\n",
      "Dataframe size (kb): 607211.492 \n",
      "\n",
      "I have made 2100 requests and downloaded 1004788 tweets\n",
      "Dataframe length: 1004788\n",
      "Dataframe last date: 2011-11-09\n",
      "Dataframe size (kb): 637759.545 \n",
      "\n",
      "I have made 2110 requests and will now sleep for 130 seconds \n",
      "\n",
      "I have made 2200 requests and downloaded 1053650 tweets\n",
      "Dataframe length: 1053650\n",
      "Dataframe last date: 2011-09-02\n",
      "Dataframe size (kb): 668016.084 \n",
      "\n",
      "I have made 2300 requests and downloaded 1102941 tweets\n",
      "Dataframe length: 1102941\n",
      "Dataframe last date: 2011-06-07\n",
      "Dataframe size (kb): 698053.355 \n",
      "\n",
      "I have made 2400 requests and downloaded 1152377 tweets\n",
      "Dataframe length: 1152377\n",
      "Dataframe last date: 2011-03-01\n",
      "Dataframe size (kb): 728060.743 \n",
      "\n",
      "I have made 2400 requests and will now sleep for 118 seconds \n",
      "\n",
      "I have made 2500 requests and downloaded 1200855 tweets\n",
      "Dataframe length: 1200855\n",
      "Dataframe last date: 2010-10-21\n",
      "Dataframe size (kb): 757825.438 \n",
      "\n",
      "I have made 2600 requests and downloaded 1250296 tweets\n",
      "Dataframe length: 1250296\n",
      "Dataframe last date: 2010-04-12\n",
      "Dataframe size (kb): 788486.558 \n",
      "\n",
      "I have made 2690 requests and will now sleep for 128 seconds \n",
      "\n",
      "I have made 2700 requests and downloaded 1298877 tweets\n",
      "Dataframe length: 1298877\n",
      "Dataframe last date: 2009-06-24\n",
      "Dataframe size (kb): 817634.133 \n",
      "\n",
      "There is no 'next_token'\n",
      "You have downloaded 1301971 tweets and the loop is now finished\n"
     ]
    }
   ],
   "source": [
    "cycle = True\n",
    "print(f'You currently have {df_merged.shape[0]} tweets in your dataframe \\n')\n",
    "\n",
    "try:\n",
    "    while cycle is True:\n",
    "        #Update the fields with the new next_token:\n",
    "        fields = fields_orig + '&next_token=' + next_token\n",
    "        \n",
    "        result = main(query, fields, headers, print_yes = False)\n",
    "        \n",
    "        #Define temporary dataframes that will be appended to the main dataframes:\n",
    "        df_temp = pd.json_normalize(result['data'])\n",
    "        df_user_temp = pd.json_normalize(result['includes']['users'])\n",
    "        df_merged_temp = merge_and_clean(df_temp, df_user_temp)\n",
    "        #Append the merged dataframe (tweets&users) to the main merged dataframe:\n",
    "        df_merged = df_merged.append(df_merged_temp, ignore_index = True)\n",
    "        \n",
    "        #Requests are limited to 1 request per 1 second:\n",
    "        time.sleep(1)\n",
    "        \n",
    "        #Update the index after making another request:\n",
    "        index += 1\n",
    "        if index%100==0:\n",
    "            print(f'I have made {index} requests and downloaded {df_merged.shape[0]} tweets')\n",
    "            print('Dataframe length:', df_merged.shape[0])\n",
    "            #Provide the last date in the dataframe to know where I'm at\n",
    "            last_date = str(df_merged.iloc[-1, df_merged.columns.get_loc('tweet_created_at')])[:10]\n",
    "            print('Dataframe last date:', last_date)\n",
    "            #Calculate the total size of the dataframe:\n",
    "            size = df_merged.memory_usage(deep=True).sum()/1000\n",
    "            print('Dataframe size (kb):', size, '\\n')\n",
    "            \n",
    "        #Check how far you are from the rate limit:    \n",
    "        if index%10==0:\n",
    "            rate_limit = getRateLimit(head\n",
    "                                      ers)\n",
    "            #If there are less than 21 requests to the limit, sleep a bit:\n",
    "            if rate_limit['remaining']<11:\n",
    "                #Calculate the sleep time:\n",
    "                sleep_time = int((rate_limit['reset']-time.time())+5)\n",
    "                print(f'I have made {index} requests and will now sleep for {int(sleep_time)} seconds \\n')\n",
    "                time.sleep(sleep_time)\n",
    "                \n",
    "        #Get the next token:\n",
    "        next_token = result['meta']['next_token']\n",
    "\n",
    "\n",
    "except KeyError as e:\n",
    "    print('There is no', str(e))\n",
    "\n",
    "finally:\n",
    "    print(f'You have downloaded {df_merged.shape[0]} tweets and the loop is now finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged.drop(['withheld.copyright', 'withheld.country_codes_x', \n",
    "                            'withheld.country_codes_y',], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged.drop(['withheld.scope', 'withheld.country_codes'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv('Datasets/banks/banks09_15.csv')\n",
    "df_merged.to_pickle('Datasets/banks/banks09_15.pickle')\n",
    "df_merged.to_feather('Datasets/banks/banks09_15.ftr')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
